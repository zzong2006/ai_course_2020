{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "기계 학습_RNN_실습",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpFize8AjlTw"
      },
      "source": [
        "# RNN 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWfqN8i2n0vR"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import time\r\n",
        "import re\r\n",
        "\r\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn6w053Qpfmj"
      },
      "source": [
        "sample_text = \"한 사람을 구함은 세상을 구하는 것입니다. \\\r\n",
        "                당신들은 기계도 짐승도 아닙니다. 사람입니다. \\\r\n",
        "                여러분의 마음속에도 인류에 대한 사랑이 숨어서 숨쉬고 있습니다.\"\r\n",
        "\r\n",
        "tf.random.set_seed(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsndSYgHMyxP"
      },
      "source": [
        "def preprocess_sentence(w):\r\n",
        "  # creating a space between a word and the punctuation following it\r\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\r\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\r\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\r\n",
        "\r\n",
        "  w = w.strip()\r\n",
        "\r\n",
        "  # adding a start and an end token to the sentence\r\n",
        "  # so that the model know when to start and stop predicting.\r\n",
        "  w = '<start> ' + w + ' <end>'\r\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pI1qcXF5M7H3",
        "outputId": "f9b918ce-326b-41fe-ef02-7cee6b2d4f27"
      },
      "source": [
        "seq_text = preprocess_sentence(sample_text)\r\n",
        "\r\n",
        "seq_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> 한 사람을 구함은 세상을 구하는 것입니다 . 당신들은 기계도 짐승도 아닙니다 . 사람입니다 . 여러분의 마음속에도 인류에 대한 사랑이 숨어서 숨쉬고 있습니다 . <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 372
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiqjF2z2NbKz"
      },
      "source": [
        "def tokenize(lang):\r\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\r\n",
        "  lang_tokenizer.fit_on_texts(lang)\r\n",
        "\r\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\r\n",
        "\r\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\r\n",
        "                                                         padding='post')\r\n",
        "\r\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhPKkvFtPKSs",
        "outputId": "8fb11513-0b17-4c26-e83b-e4bf73116055"
      },
      "source": [
        "tokenize(np.array([seq_text]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 2,  3,  4,  5,  6,  7,  8,  1,  9, 10, 11, 12,  1, 13,  1, 14,\n",
              "         15, 16, 17, 18, 19, 20, 21,  1, 22]], dtype=int32),\n",
              " <keras_preprocessing.text.Tokenizer at 0x7f96e3ad5e10>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 374
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjjghAaDNjEn",
        "outputId": "e292676f-4726-4e72-a456-7909b97be8a8"
      },
      "source": [
        "input_tensor, inp_lang_tokenizer = tokenize(np.array([seq_text]))\r\n",
        "\r\n",
        "for t in input_tensor[-1]:\r\n",
        "  if t!=0:\r\n",
        "    print (\"%d ----> %s\" % (t, inp_lang_tokenizer.index_word[t]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 ----> <start>\n",
            "3 ----> 한\n",
            "4 ----> 사람을\n",
            "5 ----> 구함은\n",
            "6 ----> 세상을\n",
            "7 ----> 구하는\n",
            "8 ----> 것입니다\n",
            "1 ----> .\n",
            "9 ----> 당신들은\n",
            "10 ----> 기계도\n",
            "11 ----> 짐승도\n",
            "12 ----> 아닙니다\n",
            "1 ----> .\n",
            "13 ----> 사람입니다\n",
            "1 ----> .\n",
            "14 ----> 여러분의\n",
            "15 ----> 마음속에도\n",
            "16 ----> 인류에\n",
            "17 ----> 대한\n",
            "18 ----> 사랑이\n",
            "19 ----> 숨어서\n",
            "20 ----> 숨쉬고\n",
            "21 ----> 있습니다\n",
            "1 ----> .\n",
            "22 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX5N20HSQoJr"
      },
      "source": [
        "def split_input_target(chunk):\r\n",
        "    input_text = chunk[:-1]\r\n",
        "    target_text = chunk[1:]\r\n",
        "    return input_text, target_text\r\n",
        "\r\n",
        "input_example, target_example = split_input_target(input_tensor[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh-ehpQ-Q5lY",
        "outputId": "459f5b20-fe5c-4b72-eb0b-2e806cc2b0d7"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\r\n",
        "    print(\"{:4d}단계\".format(i))\r\n",
        "    print(\"  입력: {} ({:s})\".format(input_idx, inp_lang_tokenizer.index_word[input_idx]))\r\n",
        "    print(\"  예상 출력: {} ({:s})\".format(target_idx, inp_lang_tokenizer.index_word[target_idx]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0단계\n",
            "  입력: 2 (<start>)\n",
            "  예상 출력: 3 (한)\n",
            "   1단계\n",
            "  입력: 3 (한)\n",
            "  예상 출력: 4 (사람을)\n",
            "   2단계\n",
            "  입력: 4 (사람을)\n",
            "  예상 출력: 5 (구함은)\n",
            "   3단계\n",
            "  입력: 5 (구함은)\n",
            "  예상 출력: 6 (세상을)\n",
            "   4단계\n",
            "  입력: 6 (세상을)\n",
            "  예상 출력: 7 (구하는)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9JiErgfSsyK",
        "outputId": "e966009b-c257-46b4-93e3-55ef3bcce1e9"
      },
      "source": [
        "inp_lang_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 1,\n",
              " '<end>': 22,\n",
              " '<start>': 2,\n",
              " '것입니다': 8,\n",
              " '구하는': 7,\n",
              " '구함은': 5,\n",
              " '기계도': 10,\n",
              " '당신들은': 9,\n",
              " '대한': 17,\n",
              " '마음속에도': 15,\n",
              " '사람을': 4,\n",
              " '사람입니다': 13,\n",
              " '사랑이': 18,\n",
              " '세상을': 6,\n",
              " '숨쉬고': 20,\n",
              " '숨어서': 19,\n",
              " '아닙니다': 12,\n",
              " '여러분의': 14,\n",
              " '인류에': 16,\n",
              " '있습니다': 21,\n",
              " '짐승도': 11,\n",
              " '한': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 378
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5PqCi_XRDcI"
      },
      "source": [
        "# 문자로 된 어휘 사전의 크기\r\n",
        "vocab_size = len(inp_lang_tokenizer.index_word) + 1\r\n",
        "\r\n",
        "# 임베딩 차원\r\n",
        "embedding_dim = 32\r\n",
        "\r\n",
        "# RNN 유닛(unit) 개수\r\n",
        "rnn_units = 256\r\n",
        "\r\n",
        "# 배치 사이즈\r\n",
        "batch_size = 1\r\n",
        "\r\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n",
        "  model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),   # faster than Dense\r\n",
        "    tf.keras.layers.SimpleRNN(rnn_units,\r\n",
        "                        return_sequences=True,\r\n",
        "                        stateful=True,\r\n",
        "                        recurrent_initializer='glorot_uniform'),\r\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\r\n",
        "  ])\r\n",
        "  return model\r\n",
        "\r\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "784HRRWEUelr",
        "outputId": "482cde9e-9871-4e2d-8f8f-aa9b7ef412d4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_24 (Embedding)     (1, None, 32)             736       \n",
            "_________________________________________________________________\n",
            "simple_rnn_18 (SimpleRNN)    (1, None, 256)            73984     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (1, None, 23)             5911      \n",
            "=================================================================\n",
            "Total params: 80,631\n",
            "Trainable params: 80,631\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBcnHFJgwWF7"
      },
      "source": [
        "### 모델 동작 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvCdxIHBU816"
      },
      "source": [
        "input_x = input_example.reshape(1, -1, 1)  # batch_size, time steps, features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCCFUGnDU3jl",
        "outputId": "0ad0511b-ecb7-4f7f-9a63-55d1af1f4d7b"
      },
      "source": [
        "example_batch_predictions = model(input_x)\r\n",
        "print(example_batch_predictions.shape, \"# (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 24, 23) # (배치 크기, 시퀀스 길이, 어휘 사전 크기)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTW0z5vAu-GN"
      },
      "source": [
        "sampled_indices = tf.random.categorical(logits=example_batch_predictions[0], num_samples=1)\r\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\r\n",
        "\r\n",
        "sampled_indices = np.where(sampled_indices != 0, sampled_indices, np.random.choice(np.arange(1, vocab_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElEm_pYzwyY7",
        "outputId": "48a6e606-6248-4979-bb7f-c1d94133b2d9"
      },
      "source": [
        "print(\"입력: \\n\", ' '.join(map(inp_lang_tokenizer.index_word.get, input_example)))\r\n",
        "print()\r\n",
        "print(\"예측된 다음 문자: \\n\", ' '.join(map(inp_lang_tokenizer.index_word.get, sampled_indices)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력: \n",
            " <start> 한 사람을 구함은 세상을 구하는 것입니다 . 당신들은 기계도 짐승도 아닙니다 . 사람입니다 . 여러분의 마음속에도 인류에 대한 사랑이 숨어서 숨쉬고 있습니다 .\n",
            "\n",
            "예측된 다음 문자: \n",
            " 구함은 <start> . 구함은 숨쉬고 구하는 당신들은 아닙니다 인류에 구하는 <start> 것입니다 <end> 세상을 구하는 인류에 . 아닙니다 아닙니다 아닙니다 숨어서 사랑이 짐승도 기계도\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJr9HnGrlYfY",
        "outputId": "96102070-83d9-4d83-e19c-ad60109939dc"
      },
      "source": [
        "input_y = target_example.reshape(1, -1, 1)  # batch_size, time steps, features\r\n",
        "\r\n",
        "def loss(labels, logits):\r\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\r\n",
        "\r\n",
        "example_batch_loss  = loss(input_y, example_batch_predictions)\r\n",
        "print(\"스칼라 손실:          \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "스칼라 손실:           3.131261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwPNwJ-pn26L"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Fsr1Tksn58g",
        "outputId": "d27a208b-f797-48ab-957c-ed50a01affbc"
      },
      "source": [
        "EPOCHS=100\r\n",
        "\r\n",
        "history = model.fit(x=input_x, y=input_y, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 834ms/step - loss: 3.1227\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.9846\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 3.0568\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.9208\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.8740\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.8677\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.8611\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.7448\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6611\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.5853\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.4277\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.8494\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.6084\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.1375\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.3786\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.4308\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.4635\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.2917\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.0612\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.7677\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.5033\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.2919\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2413\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.4503\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4318\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.4548\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.4142\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.3528\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.2830\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.2968\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.4562\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4867\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.1338\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.9512\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8811\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7256\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.6647\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5825\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6137\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.9386\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.0017\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.3079\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.6139\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6483\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4745\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6672\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.8674\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5611\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8378\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.4657\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1572\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0573\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0401\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9999\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.9507\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9283\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.9212\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8688\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7529\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6660\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6251\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5964\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5596\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5110\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4738\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4519\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4350\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4327\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4253\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3667\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2726\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2121\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1929\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1872\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1844\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1858\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1868\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1766\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1546\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1306\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1123\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1006\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0927\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0861\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0795\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0727\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0660\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0599\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0545\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0501\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0464\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0433\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0408\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0388\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0370\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0355\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0342\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0331\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0321\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J-G6q9o8gNL"
      },
      "source": [
        "![images](https://tensorflow.org/tutorials/text/images/text_generation_sampling.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwo95FMCxxWk"
      },
      "source": [
        "def generate_text(model, start_string, temperature):\r\n",
        "  # 시작 문자열을 숫자로 변환(벡터화)\r\n",
        "  input_eval = [inp_lang_tokenizer.word_index[start_string]]\r\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\r\n",
        "\r\n",
        "  # 결과를 저장할 빈 문자열\r\n",
        "  text_generated = []\r\n",
        "\r\n",
        "  model.reset_states()    # 모델 초기화\r\n",
        "  for i in range(num_generate):\r\n",
        "    predictions = model(input_eval)\r\n",
        "\r\n",
        "    predictions = tf.squeeze(predictions, 0)  # 배치 차원 제거\r\n",
        "    \r\n",
        "    # 온도가 낮으면 더 예측 가능한 텍스트가 됩니다.\r\n",
        "    # 온도가 높으면 더 의외의 텍스트가 됩니다.  \r\n",
        "    predictions = predictions / temperature   # 온도 적용\r\n",
        "\r\n",
        "    predicted_id = 0\r\n",
        "    while predicted_id == 0:\r\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n",
        "\r\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\r\n",
        "\r\n",
        "    predicted_word = inp_lang_tokenizer.index_word[predicted_id]\r\n",
        "    text_generated.append(predicted_word)\r\n",
        "\r\n",
        "    if predicted_word == \"<end>\":\r\n",
        "      break\r\n",
        "\r\n",
        "  return ' '.join(text_generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RM6WueR7KOa",
        "outputId": "47572f1c-8347-485e-ab31-c63a9a1811ef"
      },
      "source": [
        "start_string = '<start>'\r\n",
        "print('temperature=1 :', generate_text(model, start_string, temperature=1))\r\n",
        "print('temperature=0.1 :', generate_text(model, start_string, temperature=0.1))\r\n",
        "print('temperature=0.01 :', generate_text(model, start_string, temperature=0.01))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "temperature=1 : 구함은 숨어서 사람입니다 마음속에도 한 있습니다 사랑이 여러분의 구함은 구함은 당신들은 있습니다 구하는 . 짐승도 세상을 당신들은 사랑이 당신들은 <end>\n",
            "temperature=0.1 : 아닙니다 기계도 구함은 세상을 구하는 . . 당신들은 기계도 짐승도 아닙니다 . 사람입니다 . 여러분의 마음속에도 인류에 대한 사랑이 숨어서 숨쉬고 있습니다 . <end>\n",
            "temperature=0.01 : 한 사람을 구함은 세상을 구하는 것입니다 . 당신들은 기계도 짐승도 아닙니다 . 사람입니다 . 여러분의 마음속에도 인류에 대한 사랑이 숨어서 숨쉬고 있습니다 . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}